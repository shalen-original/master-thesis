
@online{niantic_inc_pokemon_nodate,
	title = {Pokémon {GO}},
	url = {https://pokemongolive.com/},
	abstract = {Join Trainers around the world and play Pokémon {GO} together in new and exciting ways. Overcome challenges, catch more Pokémon, and forge friendships through incredible shared experiences.},
	author = {{Niantic, Inc.}},
	urldate = {2022-01-05},
	langid = {english},
	file = {Snapshot:/home/shalen/Zotero/storage/TDPWAZ65/en.html:text/html},
}

@article{kocian_visually-coupled_1977,
	title = {A Visually-Coupled Airborne Systems Simulator ({VCASS}) - An Approach to Visual Simulation},
	url = {https://www.semanticscholar.org/paper/A-Visually-Coupled-Airborne-Systems-Simulator-An-to-Kocian/7012fc9e83f931df281d6143f3758f1ca2f67956},
	abstract = {This paper describes a new approach to solving the visual presentation problems of aircraft simulators by using visually coupled systems ({VCS}). For many years it has been the mission of this laboratory to optimize the visual interface of crew members to advanced weapon systems. This mission has been primarily pursued in two areas: (1) the establishment of control/display engineering criteria; and (2) the prototyping of advanced concepts for control and display interface. An important part of fulfilling this mission has been the development of {VCS} components which includes head position sensing systems or helmet mounted sights ({HMS}), eye position sensing systems ({EPS}) and helmet mounted displays ({HMD}). The author believes that the unique capabilities of a visually-coupled system ({VCS} - combination of a helmet-mounted sight and helmet- mounted display) can meet the simulation requirements as well as improve upon existing ground based simulation techniques.},
	journaltitle = {undefined},
	author = {Kocian, D.},
	urldate = {2022-01-17},
	date = {1977},
	langid = {english},
	file = {Snapshot:/home/shalen/Zotero/storage/EPAC8LQD/7012fc9e83f931df281d6143f3758f1ca2f67956.html:text/html},
}

@online{google_llc_arcore_nodate,
	title = {{ARCore}},
	url = {https://developers.google.com/ar},
	abstract = {With {ARCore}, build new augmented reality experiences that seamlessly blend the digital and physical worlds. Transform the way people play, shop, learn, create, and experience the world together—at Google scale.},
	titleaddon = {Build new augmented reality experiences that seamlessly blend the digital and physical worlds},
	author = {{Google LLC}},
	urldate = {2022-01-17},
	langid = {english},
	file = {Snapshot:/home/shalen/Zotero/storage/SS3HK39U/ar.html:text/html},
}

@online{apple_inc_arkit_nodate,
	title = {{ARKit}},
	url = {https://developer.apple.com/augmented-reality/arkit/},
	abstract = {Take advantage of the latest advances in {ARKit} to create incredible augmented reality experiences for Apple platforms.},
	titleaddon = {{ARKit} Overview - Augmented Reality},
	author = {{Apple, Inc}},
	urldate = {2022-01-17},
	langid = {english},
	file = {Snapshot:/home/shalen/Zotero/storage/FS5ZTUPD/arkit.html:text/html},
}

@online{microsoft_corporation_microsoft_nodate,
	title = {Microsoft {HoloLens}},
	url = {https://www.microsoft.com/en-us/hololens},
	abstract = {Introducing {HoloLens} 2, an untethered mixed reality headset that's designed to help you solve real business problems today using intelligent apps and solutions.},
	titleaddon = {Microsoft {HoloLens} - Mixed Reality Technology for Business},
	author = {{Microsoft Corporation}},
	urldate = {2022-01-17},
	langid = {english},
	file = {Snapshot:/home/shalen/Zotero/storage/2UIZDSW6/hololens.html:text/html},
}

@online{meta_platforms_inc_spark_nodate,
	title = {Spark {AR} Studio},
	url = {https://sparkar.facebook.com/ar-studio},
	abstract = {Create interactive augmented reality experiences with or without code, then share what you build with the world. Get started with Spark {AR} Studio now.},
	titleaddon = {Spark {AR} Studio - Create Augmented Reality Experiences {\textbar} Spark {AR} Studio},
	author = {{Meta Platforms, Inc}},
	urldate = {2022-01-17},
	langid = {english},
	file = {Snapshot:/home/shalen/Zotero/storage/VXJ9CP4C/ar-studio.html:text/html},
}

@article{andress_--fly_2018,
	title = {On-the-fly augmented reality for orthopedic surgery using a multimodal fiducial},
	volume = {5},
	issn = {2329-4302, 2329-4310},
	doi = {10.1117/1.JMI.5.2.021209},
	abstract = {Fluoroscopic x-ray guidance is a cornerstone for percutaneous orthopedic surgical procedures. However, two-dimensional (2-D) observations of the three-dimensional (3-D) anatomy suffer from the effects of projective simplification. Consequently, many x-ray images from various orientations need to be acquired for the surgeon to accurately assess the spatial relations between the patient’s anatomy and the surgical tools. We present an on-the-fly surgical support system that provides guidance using augmented reality and can be used in quasiunprepared operating rooms. The proposed system builds upon a multimodality marker and simultaneous localization and mapping technique to cocalibrate an optical see-through head mounted display to a C-arm fluoroscopy system. Then, annotations on the 2-D x-ray images can be rendered as virtual objects in 3-D providing surgical guidance. We quantitatively evaluate the components of the proposed system and, finally, design a feasibility study on a semianthropomorphic phantom. The accuracy of our system was comparable to the traditional image-guided technique while substantially reducing the number of acquired x-ray images as well as procedure time. Our promising results encourage further research on the interaction between virtual and real objects that we believe will directly benefit the proposed method. Further, we would like to explore the capabilities of our on-the-fly augmented reality support system in a larger study directed toward common orthopedic interventions.},
	pages = {021209},
	number = {2},
	journaltitle = {Journal of Medical Imaging},
	shortjournal = {{JMI}},
	author = {Andress, Sebastian and M.d, Alex Johnson and Unberath, Mathias and Winkler, Alexander F. and Yu, Kevin and Fotouhi, Javad and M.d, Simon Weidert and M.d, Greg M. Osgood and Navab, Nassir},
	urldate = {2022-01-17},
	date = {2018-01},
	note = {Publisher: {SPIE}},
	file = {Full Text PDF:/home/shalen/Zotero/storage/4NLENPXA/Andress et al. - 2018 - On-the-fly augmented reality for orthopedic surger.pdf:application/pdf;Snapshot:/home/shalen/Zotero/storage/Q5V84W8J/1.JMI.5.2.021209.html:text/html},
}

@inproceedings{fotouhi_interventional_2016,
	title = {Interventional 3D Augmented Reality for Orthopedic and Trauma Surgery},
	eventtitle = {16th Annual Meeting of the International Society for Computer Assisted Orthopedic Surgery ({CAOS})},
	author = {Fotouhi, Javad},
	date = {2016-06-09},
	file = {Full Text PDF:/home/shalen/Zotero/storage/EZHIZ2KP/Fotouhi - 2016 - Interventional 3D Augmented Reality for Orthopedic.pdf:application/pdf},
}

@article{fallavollita_desired-view--controlled_2014,
	title = {Desired-View--controlled positioning of angiographic C-arms},
	volume = {17},
	doi = {10.1007/978-3-319-10470-6_82},
	abstract = {We present the idea of a user interface concept, which resolves the challenges involved in the control of angiographic C-arms for their constant repositioning during interventions by either the surgeons or the surgical staff. Our aim is to shift the paradigm of interventional image acquisition workflow from the traditional control device interfaces to 'desired-view' control. This allows the physicians to only communicate the desired outcome of imaging, based on simulated X-rays from pre-operative {CT} or {CTA} data, while the system takes care of computing the positioning of the imaging device relative to the patient's anatomy through inverse kinematics and {CT} to patient registration. Together with our clinical partners, we evaluate the new technique using 5 patient {CTA} and their corresponding intraoperative X-ray angiography datasets.},
	pages = {659--666},
	issue = {Pt 2},
	journaltitle = {Medical image computing and computer-assisted intervention: {MICCAI} ... International Conference on Medical Image Computing and Computer-Assisted Intervention},
	shortjournal = {Med Image Comput Comput Assist Interv},
	author = {Fallavollita, Pascal and Winkler, Alexander and Habert, Severine and Wucherer, Patrick and Stefan, Philipp and Mansour, Riad and Ghotbi, Reza and Navab, Nassir},
	date = {2014},
	pmid = {25485436},
	keywords = {Algorithms, Angiography, Aortic Aneurysm, Humans, Image Enhancement, Image Interpretation, Computer-Assisted, Reproducibility of Results, Robotics, Sensitivity and Specificity, Software, Surgery, Computer-Assisted, Tomography, X-Ray Computed, User-Computer Interface},
}

@online{assistance_publique_hopitaux_de_paris_world_nodate,
	title = {World premiere for mixed reality surgery},
	url = {https://healthcare-in-europe.com/en/news/world-premiere-for-mixed-reality-surgery.html},
	abstract = {The Assistance Publique Hôpitaux de Paris ({AP}-{HP}) has organized, in partnership with {TeraRecon}, Vizua, Microsoft and Digital Evolutis, the live broadcast of the first surgery performed in the world with a collaborative platform of mixed reality at the Avicenne Hospital {AP}-{HP}, and interacting with remote doctors.},
	author = {{Assistance Publique Hôpitaux de Paris}},
	urldate = {2022-01-17},
	langid = {english},
	file = {Snapshot:/home/shalen/Zotero/storage/4CGRCGJC/world-premiere-for-mixed-reality-surgery.html:text/html},
}

@online{magic_leap_inc_magic_nodate,
	title = {Magic Leap 1},
	url = {https://www.magicleap.com/magic-leap-1},
	abstract = {Magic Leap 1 is a lightweight, wearable computer with business-ready solutions that enable remote assistance, digital work instructions, and 3D meetings.},
	author = {{Magic Leap, Inc}},
	urldate = {2022-01-17},
	langid = {english},
	file = {Snapshot:/home/shalen/Zotero/storage/49BDKLBJ/magic-leap-1.html:text/html},
}

@online{microsoft_corporation_remote_nodate,
	title = {Remote Assist},
	url = {https://dynamics.microsoft.com/en-us/mixed-reality/remote-assist/},
	titleaddon = {Remote Assist, Microsoft Dynamics 365},
	author = {{Microsoft Corporation}},
	urldate = {2022-01-17},
	file = {Remote Assist | Microsoft Dynamics 365:/home/shalen/Zotero/storage/89LJJQ9X/remote-assist.html:text/html},
}

@online{microsoft_corporation_apps_nodate,
	title = {Apps, Services, and Solutions for {HoloLens} 2 {\textbar} Microsoft {HoloLens}},
	url = {https://www.microsoft.com/en-us/hololens/apps},
	abstract = {Explore {HoloLens} 2 apps and services for empowering your workforce, modernizing your industry, and developing custom mixed reality solutions.},
	author = {{Microsoft Corporation}},
	urldate = {2022-01-17},
	langid = {english},
	file = {Snapshot:/home/shalen/Zotero/storage/NURBYV46/apps.html:text/html},
}

@online{world_wide_web_consortium_webxr_nodate,
	title = {{WebXR} Device {API}},
	url = {https://www.w3.org/TR/webxr/},
	author = {{World Wide Web Consortium}},
	urldate = {2022-01-26},
	file = {WebXR Device API:/home/shalen/Zotero/storage/PLU4CF3B/webxr.html:text/html},
}

@online{unity_technologies_unity_nodate,
	title = {Unity Real-Time Development Platform {\textbar} 3D, 2D {VR} \& {AR} Engine},
	url = {https://unity.com/},
	abstract = {Make real-time 3D projects for Games, Animation, Film, Automotive, Transportation, Architecture, Engineering, Manufacturing \& Construction. Visualize \& simulate industrial projects in 3D, {AR}, \& {VR}.},
	author = {{Unity Technologies}},
	urldate = {2022-01-26},
	langid = {english},
	file = {Snapshot:/home/shalen/Zotero/storage/45SCHQ3H/unity.com.html:text/html},
}

@inproceedings{viertler_requirements_2015,
	title = {Requirements and Design Challenges in Rotorcraft Flight Simulations for Research Applications},
	doi = {10.2514/6.2015-1808},
	abstract = {After a discussion of the challenges in high fidelity real-time flight simulation, e. Research flight simulators often differ in requirements and design parameters compared to flight simulation training devices due to their broad field of applications. Flexibility and extensibility are the key factors to be able to use the simulation environment for numerous projects to save budget and to increase simulation fidelity over time. With design and implementation examples of the Rotorcraft Simulation Environment ({ROSIE}) at the Institute of Helicopter Technology at the Technische Universitaet Muenchen, compliance with these requirements shall be demonstrated. After a discussion of the challenges in high fidelity real-time flight simulation, e. g. time delays and resolution of visual cue information, current research projects performed with {ROSIE} are illustrated.},
	author = {Viertler, F. and Hajek, M.},
	date = {2015},
}

@inproceedings{turini_microsoft_2018,
	location = {Cham},
	title = {A Microsoft {HoloLens} Mixed Reality Surgical Simulator for Patient-Specific Hip Arthroplasty Training},
	isbn = {978-3-319-95282-6},
	doi = {10.1007/978-3-319-95282-6_15},
	series = {Lecture Notes in Computer Science},
	abstract = {Surgical simulation can offer novice surgeons an opportunity to practice skills outside the operating theatre in a safe controlled environment. According to literature evidence, nowadays there are very few training simulators available for Hip Arthroplasty ({HA}).In a previous study we have presented a physical simulator based on a lower torso phantom including a patient-specific hemi-pelvis replica embedded in a soft synthetic foam. This work explores the use of Microsoft {HoloLens} technology to enrich the physical patient-specific simulation with the implementation of wearable mixed reality functionalities. Our {HA} multimodal simulator based on mixed reality using the {HoloLens} is described by illustrating the overall system, and by summarizing the main phases of the design and development.Finally, we present a preliminary qualitative study with seven subjects (5 medical students, and 2 orthopedic surgeons) showing encouraging results that suggest the suitability of the {HoloLens} for the proposed application. However, further studies need to be conducted to perform a quantitative test of the registration accuracy of the virtual content, and to confirm qualitative results in a larger cohort of subjects.},
	pages = {201--210},
	booktitle = {Augmented Reality, Virtual Reality, and Computer Graphics},
	publisher = {Springer International Publishing},
	author = {Turini, Giuseppe and Condino, Sara and Parchi, Paolo Domenico and Viglialoro, Rosanna Maria and Piolanti, Nicola and Gesi, Marco and Ferrari, Mauro and Ferrari, Vincenzo},
	editor = {De Paolis, Lucio Tommaso and Bourdot, Patrick},
	date = {2018},
	langid = {english},
	keywords = {Augmented reality, Hip arthroplasty, Microsoft {HoloLens}, Surgical simulation},
	file = {Submitted Version:/home/shalen/Zotero/storage/Z5QQAQHF/Turini et al. - 2018 - A Microsoft HoloLens Mixed Reality Surgical Simula.pdf:application/pdf},
}

@article{walko_increasing_2021,
	title = {Increasing helicopter flight safety in maritime operations with a head-mounted display},
	volume = {12},
	issn = {1869-5590},
	doi = {10.1007/s13272-020-00474-7},
	abstract = {To increase flight safety and operational availability for helicopters, the potential benefits of helmet-mounted displays ({HMD}) are investigated, with a focus on maritime flight operations. Helicopters have long downtimes, due to harsh weather conditions or other visual impairments, especially in maritime scenarios. Flying in these poor conditions can drastically reduce flight safety. It is often difficult to recognize the horizon due to sea fog, and the absence of reference objects can complicate the maritime flight. These conditions and especially the downtimes cost money or, at worst, life’s. Therefore, {DLR} integrated the augmented reality glasses Microsoft {HoloLens} into {DLR}’s simulator {AVES} to use it as {HMD} for pilots. Subsequently, displays and symbology were developed and evaluated. To carry out a piloted simulator study, a maritime scenario was created to measure changes in the pilots’ performance with the {HMD}, like workload or situational awareness. The paper focuses (a) on the integration of the {HoloLens} into the simulator with its challenges, solutions and findings, (b) on the symbology and (c) on the piloted simulator study. Both the quality of the {HoloLens} as {HMD} and the study results are very positive. The pilots rated high usability, reduced workload, increased situational awareness and increased safety.},
	pages = {29--41},
	number = {1},
	journaltitle = {{CEAS} Aeronautical Journal},
	shortjournal = {{CEAS} Aeronaut J},
	author = {Walko, Christian and Schuchardt, Bianca},
	date = {2021-01-01},
	langid = {english},
}

@inproceedings{tran_single_2018,
	title = {Single Pilot Operations with {AR}-Glasses using Microsoft {HoloLens}},
	doi = {10.1109/DASC.2018.8569261},
	abstract = {This paper presents a conceptual design of an additional assistant system with {AR}-Glasses for single-pilot operations of commercial transport aircraft. In particular, an analysis regarding the use of the Microsoft {HoloLens} as a guidance device and co-pilot compensation, to perform useful and timely alerts based on airline emergency procedures. The augmented reality ({AR}) device was introduced to raise the ability to handle complex work flows under time pressure by adding artificial visual guidance to the crew member. The goal was to enhance situation awareness and confidence taking carefully time critical decisions without the support of a second crew member. The test scenario included an engine fire in a single cockpit operation environment during cruise flight. For the survey, 24 experienced and type rated active Airbus A320 pilots have been compared in two groups to each other using an Airbus A320 fix based simulator. Within the test scenario, the reaction time initiating the standardized action items (based on Airbus A320 quick reference handbook) have been recorded. The first group had only support by the airplane-side systems (e.g. {ECAM} display), while the second group used the {AR}-glasses. Compared to the conventional airplane-side systems support to deal with engine fire situations, direct visual instructions have been given by the {AR}-glasses and have been always linked to a displayed three-dimensional visual marker. This should immediately inform the pilot where the focus of attention must be on and thus ensure that the actions are carried out more quickly. By guiding the pilot through visual instructions, manual scanning of the cockpit parameters have been unnecessary. The direct instructions in the glasses and the additional information expected to shorten the reaction time. At the same time this should relieved the pilot and reduce the workload in order to get a more efficient human machine system and enhanced situation awareness.},
	eventtitle = {2018 {IEEE}/{AIAA} 37th Digital Avionics Systems Conference ({DASC})},
	pages = {1--7},
	booktitle = {2018 {IEEE}/{AIAA} 37th Digital Avionics Systems Conference ({DASC})},
	author = {Tran, The Huy and Behrend, Ferdinand and Fünning, Niels and Arango, Andres},
	date = {2018-09},
	note = {{ISSN}: 2155-7209},
	keywords = {Augmented reality, Aircraft, augmented reality, Engines, Fires, Glass, Microsoft Hololens, single-pilot operations, Task analysis, virtual assistant, Visualization},
	file = {IEEE Xplore Abstract Record:/home/shalen/Zotero/storage/W94IJ9EH/8569261.html:text/html},
}

@article{brooks_no_1987,
	title = {No Silver Bullet Essence and Accidents of Software Engineering},
	volume = {20},
	issn = {1558-0814},
	doi = {10.1109/MC.1987.1663532},
	pages = {10--19},
	number = {4},
	journaltitle = {Computer},
	author = {Brooks, Frederick},
	date = {1987-04},
	note = {Conference Name: Computer},
	keywords = {Computer industry, Costs, Diseases, Hardware, Industrial accidents, Project management, Roads, Silver, Software engineering, Technological innovation},
	file = {IEEE Xplore Abstract Record:/home/shalen/Zotero/storage/5YPE2SV4/1663532.html:text/html},
}

@article{walko_integration_2020,
	title = {Integration and use of an augmented reality display in a maritime helicopter simulator},
	volume = {59},
	issn = {0091-3286},
	doi = {10.1117/1.OE.59.4.043104},
	pages = {1},
	number = {4},
	journaltitle = {Optical Engineering},
	shortjournal = {Opt. Eng.},
	author = {Walko, Christian and Peinecke, Niklas},
	date = {2020-04-28},
}

@online{microsoft_corporation_hololens_nodate,
	title = {{HoloLens} 2 Moving Platform Mode},
	url = {https://docs.microsoft.com/en-us/hololens/hololens2-moving-platform},
	abstract = {How to use {HoloLens} on moving platforms},
	author = {{Microsoft Corporation}},
	urldate = {2022-01-26},
	langid = {english},
	file = {Snapshot:/home/shalen/Zotero/storage/6BQ8ZMVL/hololens2-moving-platform.html:text/html},
}

@inproceedings{martin-gomez_augmented_2020,
	title = {Augmented Mirrors},
	doi = {10.1109/ISMAR50242.2020.00045},
	abstract = {A recurrent problem in egocentric Augmented Reality ({AR}) applications is the misestimation of depth. Providing alternative views from non-egocentric perspectives can convey useful information for applications that require the correct judgment of depth as it is in the case of placement and alignment of virtual and real content, but also for exploration and visualization tasks.In this paper, we introduce Augmented Mirrors. Through the integration of a real mirror, our approach is capable to reflect changes of the real and virtual content of an {AR} application while users benefit from the perceptual advantages of using mirrors. Our concept, simple yet effective, only requires tracking the user and mirror poses with the accuracy demanded by a specific application. To showcase the potential and flexibility of the Augmented Mirrors, we present and discuss multiple examples ranging from alignment, exploration, spatial understanding, and selective content visualization using different {AR}-enabled devices and tracking technologies. We envision the Augmented Mirrors as a new and valuable concept that can be used in applications that benefit from additional viewpoints and require the simultaneous visualization of real and virtual content.},
	eventtitle = {2020 {IEEE} International Symposium on Mixed and Augmented Reality ({ISMAR})},
	pages = {217--226},
	booktitle = {2020 {IEEE} International Symposium on Mixed and Augmented Reality ({ISMAR})},
	author = {Martin-Gomez, Alejandro and Winkler, Alexander and Yu, Kevin and Roth, Daniel and Eck, Ulrich and Navab, Nassir},
	date = {2020-11},
	note = {{ISSN}: 1554-7868},
	keywords = {Augmented reality, Task analysis, Visualization, concepts and models—, concepts and paradigms—, Human-centered computing Visualization—Visualization—Visualization theory, Human-centered computing—Human computer interaction ({HCI})— {HCI} theory, Human-centered computing—Human computer interaction ({HCI})—Interaction paradigms Mixed / augmented reality—, Manifolds, Mirrors, Synchronization, Tools},
	file = {IEEE Xplore Abstract Record:/home/shalen/Zotero/storage/8RI8L8TS/9284798.html:text/html},
}

@online{ptc_inc_vuforia_nodate,
	title = {Vuforia Enterprise Augmented Reality Software},
	url = {https://www.ptc.com/en/products/vuforia},
	abstract = {Learn why more market leaders trust the expansive capabilities of Vuforia’s enterprise augmented reality software to build solutions and transform their business.},
	author = {{PTC Inc.}},
	urldate = {2022-01-28},
	langid = {english},
	file = {Snapshot:/home/shalen/Zotero/storage/MT3ZW6DW/vuforia.html:text/html},
}

@online{microsoft_corporation_world_nodate,
	title = {World Locking Tools},
	url = {https://microsoft.github.io/MixedReality-WorldLockingTools-Unity/README.html},
	author = {{Microsoft Corporation}},
	urldate = {2022-01-28},
	file = {Welcome! | World Locking Tools for Unity Documentation:/home/shalen/Zotero/storage/IM2XYAUS/README.html:text/html},
}

@article{lowe_distinctive_2004,
	title = {Distinctive Image Features from Scale-Invariant Keypoints},
	volume = {60},
	issn = {1573-1405},
	url = {https://doi.org/10.1023/B:VISI.0000029664.99615.94},
	doi = {10.1023/B:VISI.0000029664.99615.94},
	abstract = {This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene. The features are invariant to image scale and rotation, and are shown to provide robust matching across a substantial range of affine distortion, change in 3D viewpoint, addition of noise, and change in illumination. The features are highly distinctive, in the sense that a single feature can be correctly matched with high probability against a large database of features from many images. This paper also describes an approach to using these features for object recognition. The recognition proceeds by matching individual features to a database of features from known objects using a fast nearest-neighbor algorithm, followed by a Hough transform to identify clusters belonging to a single object, and finally performing verification through least-squares solution for consistent pose parameters. This approach to recognition can robustly identify objects among clutter and occlusion while achieving near real-time performance.},
	pages = {91--110},
	number = {2},
	journaltitle = {International Journal of Computer Vision},
	shortjournal = {International Journal of Computer Vision},
	author = {Lowe, David G.},
	urldate = {2022-01-28},
	date = {2004-11-01},
	langid = {english},
}

@inproceedings{alcantarilla_fast_2013,
	title = {Fast Explicit Diffusion for Accelerated Features in Nonlinear Scale Spaces},
	doi = {10.5244/C.27.13},
	abstract = {A novel and fast multiscale feature detection and description approach that exploits the benefits of nonlinear scale spaces and introduces a Modified-Local Difference Binary (M-{LDB}) descriptor that is highly efficient, exploits gradient information from the non linear scale space, is scale and rotation invariant and has low storage requirements. We propose a novel and fast multiscale feature detection and description approach that exploits the benefits of nonlinear scale spaces. Previous attempts to detect and describe features in nonlinear scale spaces such as {KAZE} [1] and {BFSIFT} [6] are highly time consuming due to the computational burden of creating the nonlinear scale space. In this paper we propose to use recent numerical schemes called Fast Explicit Diffusion ({FED}) [3, 4] embedded in a pyramidal framework to dramatically speed-up feature detection in nonlinear scale spaces. In addition, we introduce a Modified-Local Difference Binary (M-{LDB}) descriptor that is highly efficient, exploits gradient information from the nonlinear scale space, is scale and rotation invariant and has low storage requirements. Our features are called Accelerated-{KAZE} (A-{KAZE}) due to the dramatic speed-up introduced by {FED} schemes embedded in a pyramidal framework.},
	booktitle = {{BMVC}},
	author = {Alcantarilla, P. and Nuevo, J. and Bartoli, A.},
	date = {2013},
}

@online{alicevision_meshroom_nodate,
	title = {Meshroom - 3D Reconstruction Software},
	url = {https://alicevision.org/#meshroom},
	author = {{AliceVision}},
	urldate = {2022-01-28},
	file = {AliceVision | Meshroom - 3D Reconstruction Software:/home/shalen/Zotero/storage/TYQB7BGX/alicevision.org.html:text/html},
}

@article{knapitsch_tanks_2017,
	title = {Tanks and temples: benchmarking large-scale scene reconstruction},
	volume = {36},
	issn = {0730-0301},
	url = {https://doi.org/10.1145/3072959.3073599},
	doi = {10.1145/3072959.3073599},
	shorttitle = {Tanks and temples},
	abstract = {We present a benchmark for image-based 3D reconstruction. The benchmark sequences were acquired outside the lab, in realistic conditions. Ground-truth data was captured using an industrial laser scanner. The benchmark includes both outdoor scenes and indoor environments. High-resolution video sequences are provided as input, supporting the development of novel pipelines that take advantage of video input to increase reconstruction fidelity. We report the performance of many image-based 3D reconstruction pipelines on the new benchmark. The results point to exciting challenges and opportunities for future work.},
	pages = {78:1--78:13},
	number = {4},
	journaltitle = {{ACM} Transactions on Graphics},
	shortjournal = {{ACM} Trans. Graph.},
	author = {Knapitsch, Arno and Park, Jaesik and Zhou, Qian-Yi and Koltun, Vladlen},
	urldate = {2022-01-28},
	date = {2017-07-20},
	keywords = {image-based reconstruction, large-scale scene reconstruction, multi-view stereo, structure from motion},
}

@online{apple_inc_ipad_nodate,
	title = {{iPad} Pro 12.9 (5th generation)},
	url = {https://www.apple.com/ipad-pro/specs/},
	titleaddon = {Apple},
	author = {{Apple, Inc}},
	urldate = {2022-01-28},
	langid = {american},
	file = {Snapshot:/home/shalen/Zotero/storage/YE8NT7YR/specs.html:text/html},
}

@article{cignoni_meshlab_2008,
	title = {{MeshLab}: an Open-Source Mesh Processing Tool},
	url = {http://diglib.eg.org/handle/10.2312/LocalChapterEvents.ItalChap.ItalianChapConf2008.129-136},
	doi = {10.2312/LOCALCHAPTEREVENTS/ITALCHAP/ITALIANCHAPCONF2008/129-136},
	shorttitle = {{MeshLab}},
	abstract = {The paper presents {MeshLab}, an open source, extensible, mesh processing system that has been developed at the Visual Computing Lab of the {ISTI}-{CNR} with the helps of tens of students. We will describe the {MeshLab} architecture, its main features and design objectives discussing what strategies have been used to support its development. Various examples of the practical uses of {MeshLab} in research and professional frameworks are reported to show the various capabilities of the presented system.},
	pages = {8 pages},
	journaltitle = {Eurographics Italian Chapter Conference},
	author = {Cignoni, Paolo and Callieri, Marco and Corsini, Massimiliano and Dellepiane, Matteo and Ganovelli, Fabio and Ranzuglia, Guido},
	urldate = {2022-01-28},
	date = {2008},
	langid = {english},
	note = {Artwork Size: 8 pages
{ISBN}: 9783905673685
Publisher: The Eurographics Association},
	keywords = {Categories and Subject Descriptors (according to {ACM} {CCS}): I.3.3 [Computer Graphics]: Line and Curve Generation},
}

@article{zhang_fast_2021,
	title = {Fast and Robust Iterative Closest Point},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2021.3054619},
	abstract = {The Iterative Closest Point ({ICP}) algorithm and its variants are a fundamental technique for rigid registration between two point sets, with wide applications in different areas from robotics to 3D reconstruction. The main drawbacks for {ICP} are its slow convergence as well as its sensitivity to outliers, missing data, and partial overlaps. Recent work such as Sparse {ICP} achieves robustness via sparsity optimization at the cost of computational speed. In this paper, we propose a new method for robust registration with fast convergence. First, we show that the classical point-to-point {ICP} can be treated as a majorization-minimization ({MM}) algorithm, and propose an Anderson acceleration approach to speed up its convergence. In addition, we introduce a robust error metric based on the Welsch's function, which is minimized efficiently using the {MM} algorithm with Anderson acceleration. On challenging datasets with noises and partial overlaps, we achieve similar or better accuracy than Sparse {ICP} while being at least an order of magnitude faster. Finally, we extend the robust formulation to point-to-plane {ICP}, and solve the resulting problem using a similar Anderson-accelerated {MM} strategy. Our robust {ICP} methods improve the registration accuracy on benchmark datasets while being competitive in computational time.},
	pages = {1--1},
	journaltitle = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	author = {Zhang, Juyong and Yao, Yuxin and Deng, Bailin},
	date = {2021},
	note = {Conference Name: {IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Acceleration, Anderson Acceleration, Convergence, Fixed-point iterations, Iterative closest point algorithm, Majorlazer Minimization method, Measurement, Optimization, Rigid Registration, Robust Estimator, Robustness, Three-dimensional displays},
	file = {Accepted Version:/home/shalen/Zotero/storage/LT2B5W3U/Zhang et al. - 2021 - Fast and Robust Iterative Closest Point.pdf:application/pdf;IEEE Xplore Abstract Record:/home/shalen/Zotero/storage/RSMYS7BX/9336308.html:text/html},
}

@online{blender_foundation_blender_nodate,
	title = {The Blender Project - Free and Open 3D Creation Software},
	url = {https://www.blender.org/},
	author = {{Blender Foundation}},
	urldate = {2022-01-28},
	langid = {english},
	file = {Snapshot:/home/shalen/Zotero/storage/8VP38P8L/www.blender.org.html:text/html},
}

@online{iogp_geomatics_committee_epsg_nodate,
	title = {{EPSG} Geodetic Parameter Dataset},
	url = {https://epsg.org/home.html},
	author = {{IOGP Geomatics Committee}},
	urldate = {2022-01-29},
	file = {EPSG Geodetic Parameter Dataset:/home/shalen/Zotero/storage/4YMEJWBV/home.html:text/html},
}

@inreference{noauthor_local_2022,
	title = {Local tangent plane coordinates},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Local_tangent_plane_coordinates&oldid=1063628292},
	abstract = {Local tangent plane coordinates ({LTP}), also known as local ellipsoidal system, local geodetic coordinate system, or local vertical, local horizontal coordinates ({LVLH}), are a spatial reference system based on the tangent plane defined by the local vertical direction and the Earth's axis of rotation.
It consists of three coordinates: one represents the position along the northern axis, one along the local eastern axis, and one represents the vertical position.  
Two right-handed variants exist: east, north, up ({ENU}) coordinates and north, east, down ({NED}) coordinates.
They serve for representing state vectors that are commonly used in aviation and marine cybernetics.},
	booktitle = {Wikipedia},
	urldate = {2022-01-30},
	date = {2022-01-04},
	langid = {english},
	note = {Page Version {ID}: 1063628292},
	file = {Snapshot:/home/shalen/Zotero/storage/IF54UG4I/Local_tangent_plane_coordinates.html:text/html},
}

@online{noauthor_pyproj_nodate,
	title = {{PyProj} - Python interface to {PROJ}},
	url = {https://pyproj4.github.io/pyproj/stable/},
	urldate = {2022-01-30},
	file = {pyproj Documentation — pyproj 3.3.0 documentation:/home/shalen/Zotero/storage/HMNRCJLD/stable.html:text/html},
}

@online{qgis_development_team_qgis_nodate,
	title = {{QGIS} - A Free and Open Source Geographic Information System},
	url = {https://qgis.org/en/site/},
	author = {{QGIS Development Team}},
	urldate = {2022-01-30},
	file = {Welcome to the QGIS project!:/home/shalen/Zotero/storage/6YIKFNWV/site.html:text/html},
}

@thesis{zintl_design_2020,
	location = {Munich},
	title = {Design and Implementation of a Modular Primary Flight Display for a Flight Simulator},
	abstract = {This master’s thesis describes the design and implementation of a highly flexible and
modular Primary Flight Display ({PFD}) for a flight simulator. Specifically, the {PFD} is
developed for the Research Flight Simulator ({RFS}) of the Institute of Flight System
Dynamics ({FSD}) at the Technical University of Munich allows researchers to perform
flight experiments in a simulated environment. A newly developed generic cockpit
provides the hardware components to simulate arbitrary aircraft configurations including
aspects as the number of engines or available flap settings. However, the current
instrumentation (e.g. {PFD}, navigation display, etc.) does not support such changes. As
a first step, it was decided to develop a suitable {PFD}. For the requirements elicitation,
existing literature on other simulators and regulatory constraints were considered.
Additionally, interviews with pilots and researchers were designed and conducted.
The interview results supplied the most frequently and urgently requested features,
thereby defining the minimum scope and priority for the implementation. A modular
approach permits an easy integration of the {PFD} into other projects and increases
maintainability for future development. As part of this thesis, a Synthetic Vision System
({SVS}) was implemented that renders visual flight paths. This {SVS} and the indicators
of the {PFD} can be placed above any external vision with known properties. The {PFD}
features many indicators found in modern displays, with the implementation featuring
multiple common designs that can be changed even during run-time. Furthermore, a
scaling mechanism for the indications in the {PFD} allows for the display to be adapted
to the aircraft type under consideration. For the evaluation, several flight tests and
interviews were conducted with professional pilots and researchers. This evaluation
process was used to verify the correct implementation of the {PFD} and most feedback was
incorporated back into the design. The {PFD} can now be used in the {RFS} or other flight
simulators for experiments and display testing. Most parts of the {PFD} have already been
integrated into a Head-Up Display ({HUD}) project for another study during this thesis.},
	pagetotal = {167},
	institution = {Technical University of Munich},
	type = {Master’s Thesis in Informatics},
	author = {Zintl, Michael},
	date = {2020-07-15},
}

@online{karlton_twohardthings_nodate,
	title = {{TwoHardThings}},
	url = {https://martinfowler.com/bliki/TwoHardThings.html},
	abstract = {There are only two hard things in Computer Science: cache invalidation and naming things -- Phil Karlton (bonus variations on the page)},
	author = {Karlton, Phil and Fowler, Martin},
	urldate = {2022-01-30},
	file = {Snapshot:/home/shalen/Zotero/storage/L3PY9AEV/TwoHardThings.html:text/html},
}

@online{microsoft_corporation_hologram_nodate,
	title = {Hologram stability - Mixed Reality},
	url = {https://docs.microsoft.com/en-us/windows/mixed-reality/develop/advanced-concepts/hologram-stability},
	author = {{Microsoft Corporation}},
	urldate = {2022-01-30},
	langid = {english},
	file = {Snapshot:/home/shalen/Zotero/storage/CSRBN6QX/hologram-stability.html:text/html},
}

@inreference{noauthor_quaternions_2022,
	title = {Quaternions and spatial rotation},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Quaternions_and_spatial_rotation&oldid=1067865383},
	booktitle = {Wikipedia},
	urldate = {2022-01-30},
	date = {2022-01-25},
	langid = {english},
	note = {Page Version {ID}: 1067865383},
	file = {Snapshot:/home/shalen/Zotero/storage/3VH54J3J/Quaternions_and_spatial_rotation.html:text/html},
}

@inreference{noauthor_distance_2021,
	title = {Distance from a point to a line},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Distance_from_a_point_to_a_line&oldid=1061784220},
	booktitle = {Wikipedia},
	urldate = {2022-01-30},
	date = {2021-12-23},
	langid = {english},
	note = {Page Version {ID}: 1061784220},
	file = {Snapshot:/home/shalen/Zotero/storage/V4N8NZG4/Distance_from_a_point_to_a_line.html:text/html},
}

@online{khronos_group_depth_nodate,
	title = {Depth Test - {OpenGL} Wiki},
	url = {https://www.khronos.org/opengl/wiki/Depth_Test},
	author = {{Khronos Group}},
	urldate = {2022-01-31},
	file = {Depth Test - OpenGL Wiki:/home/shalen/Zotero/storage/SJ52P23F/Depth_Test.html:text/html},
}

@online{gabriel_rise_1991,
	title = {The Rise of 'Worse is Better'},
	url = {https://web.stanford.edu/class/cs240/old/sp2014/readings/worse-is-better.html},
	author = {Gabriel, Richard},
	urldate = {2022-01-31},
	date = {1991},
	file = {The Rise of ``Worse is Better'':/home/shalen/Zotero/storage/V8TEB8ZN/worse-is-better.html:text/html},
}

@online{wavefront_technologies_wavefront_nodate,
	title = {Wavefront Object Files (.obj)},
	url = {http://paulbourke.net/dataformats/obj/},
	author = {{Wavefront Technologies}},
	urldate = {2022-02-03},
	file = {Object Files (.obj):/home/shalen/Zotero/storage/6BFA6PII/obj.html:text/html},
}

@misc{japan_industrial_standards_microqr_nodate,
	title = {{MicroQR} Code. {JIS} X 0510 standard.},
	url = {https://www.qrcode.com/en/codes/microqr.html},
	author = {{Japan Industrial Standards}},
	urldate = {2022-02-03},
}

@video{marsh_paved_nodate,
	location = {Austin, Texas},
	title = {The Paved Road at Netflix: At the junction of freedom and responsibility},
	url = {https://www.oreilly.com/library/view/oscon-2017/9781491976227/video306724.html},
	author = {Marsh, Dianne},
	urldate = {2022-02-03},
}

@online{noauthor_python_nodate,
	title = {The Python Programming Language},
	url = {https://www.python.org/},
	urldate = {2022-02-03},
	langid = {english},
	file = {Snapshot:/home/shalen/Zotero/storage/T5A2B66Z/www.python.org.html:text/html},
}

@online{noauthor_pypi_nodate,
	title = {{PyPI} · The Python Package Index},
	url = {https://pypi.org/},
	urldate = {2022-02-03},
	file = {PyPI · The Python Package Index:/home/shalen/Zotero/storage/RA8Q3J2N/pypi.org.html:text/html},
}

@software{noauthor_numpy_nodate,
	title = {{NumPy} - The fundamental package for array computing with Python},
	rights = {{BSD} License},
	url = {https://www.numpy.org},
	shorttitle = {numpy},
	version = {1.22.1},
	urldate = {2022-02-03},
	keywords = {Scientific/Engineering, Software Development},
	file = {Snapshot:/home/shalen/Zotero/storage/D9ST3JJN/numpy.html:text/html},
}

@software{noauthor_scipy_nodate,
	title = {{SciPy} - Scientific Library for Python},
	rights = {{BSD} License},
	url = {https://www.scipy.org},
	shorttitle = {scipy},
	version = {1.7.3},
	urldate = {2022-02-03},
	keywords = {Scientific/Engineering, Software Development - Libraries},
	file = {Snapshot:/home/shalen/Zotero/storage/CEMH9KEN/scipy.html:text/html},
}

@software{rhodes_skyfield_nodate,
	title = {Skyfield - Elegant astronomy for Python},
	rights = {{MIT} License},
	url = {http://github.com/brandon-rhodes/python-skyfield/},
	shorttitle = {skyfield},
	version = {1.41},
	author = {Rhodes, Brandon},
	urldate = {2022-02-03},
	keywords = {Scientific/Engineering - Astronomy},
	file = {Snapshot:/home/shalen/Zotero/storage/3YGCQ82U/skyfield.html:text/html},
}

@software{noauthor_matplotlib_nodate,
	title = {Matplotlib - Python plotting package},
	rights = {Python Software Foundation License},
	url = {https://matplotlib.org},
	shorttitle = {matplotlib},
	version = {3.5.1},
	urldate = {2022-02-03},
	keywords = {Scientific/Engineering - Visualization},
	file = {Snapshot:/home/shalen/Zotero/storage/JSFKPVRQ/matplotlib.html:text/html},
}

@online{noauthor_pytorch_nodate,
	title = {{PyTorch} - A Python deep learning framework},
	url = {https://pypi.org/project/torch/},
	urldate = {2022-02-03},
	file = {torch · PyPI:/home/shalen/Zotero/storage/59HD7B4G/torch.html:text/html},
}

@online{noauthor_stack_nodate,
	title = {Stack Overflow Developer Survey 2021},
	url = {https://insights.stackoverflow.com/survey/2021/#programming-scripting-and-markup-languages},
	abstract = {In May 2021 over 80,000 developers told us how they learn and level up, which tools they’re using, and what they want.},
	titleaddon = {Stack Overflow},
	urldate = {2022-02-03},
	langid = {english},
	file = {Snapshot:/home/shalen/Zotero/storage/W4FHEGDN/2021.html:text/html},
}

@online{vacc_austria_lowi_nodate,
	title = {{LOWI} Charts},
	url = {https://www.vacc-austria.org/index.php?page=content/chartlist&icao=lowi},
	author = {{VACC Austria}},
	urldate = {2022-02-07},
	file = {VACC Austria - LOWI Charts:/home/shalen/Zotero/storage/RFQPGURQ/index.html:text/html},
}

@report{butler_geojson_2016,
	title = {The {GeoJSON} Format},
	url = {https://datatracker.ietf.org/doc/rfc7946},
	abstract = {{GeoJSON} is a geospatial data interchange format based on {JavaScript} Object Notation ({JSON}). It defines several types of {JSON} objects and the manner in which they are combined to represent data about geographic features, their properties, and their spatial extents. {GeoJSON} uses a geographic coordinate reference system, World Geodetic System 1984, and units of decimal degrees.},
	number = {{RFC} 7946},
	institution = {Internet Engineering Task Force},
	type = {Request for Comments},
	author = {Butler, H. and Daly, M. and Doyle, A. and Gillies, Sean and Schaub, T. and Hagen, Stefan},
	urldate = {2022-02-07},
	date = {2016-08},
	doi = {10.17487/RFC7946},
	note = {Num Pages: 28},
	file = {Full Text PDF:/home/shalen/Zotero/storage/4GA7XE43/Butler et al. - 2016 - The GeoJSON Format.pdf:application/pdf},
}

@online{noauthor_geojson_nodate,
	title = {geojson},
	url = {https://pypi.org/project/geojson/},
	urldate = {2022-02-07},
	file = {geojson · PyPI:/home/shalen/Zotero/storage/4IEDV3WK/geojson.html:text/html},
}

@online{land_karnten_digitales_nodate,
	title = {Digitales 10m Geländemodell aus Airborne Laserscan Daten in der Projektion {EPSG}:31287 (Lambert)},
	url = {https://www.data.gv.at/katalog/dataset/b5de6975-417b-4320-afdb-eb2a9e2a1dbf},
	abstract = {Digitales Geländemodell aus Airborne Laserscan Daten. Höhenangaben des Geländes im Raster von 10m x 10m. In der Zip Datei sind die Daten im Format {GeoTIFF} enthalten. (Projektion {EPSG}:31287)},
	author = {{Land Kärnten}},
	urldate = {2022-02-07},
	langid = {german},
	file = {Snapshot:/home/shalen/Zotero/storage/7C5N8T8F/b5de6975-417b-4320-afdb-eb2a9e2a1dbf.html:text/html},
}

@misc{the_open_geospatial_consortium_ogc_2019,
	title = {{OGC} {GeoTIFF} Standard},
	url = {https://earthdata.nasa.gov/esdis/eso/standards-and-references/geotiff},
	abstract = {{OGC} {GeoTIFF} Standard is an {OGC}® Implementation Standard. {GeoTIFF} is based on the {TIFF} format and is used as an interchange format for georeferenced raster imagery. {GeoTIFF} is in wide use in {NASA} Earth science data systems.},
	author = {{The Open Geospatial Consortium}},
	urldate = {2022-02-07},
	date = {2019},
}

@report{bray_javascript_2017,
	title = {The {JavaScript} Object Notation ({JSON}) Data Interchange Format},
	url = {https://datatracker.ietf.org/doc/rfc8259},
	abstract = {{JavaScript} Object Notation ({JSON}) is a lightweight, text-based, language-independent data interchange format. It was derived from the {ECMAScript} Programming Language Standard. {JSON} defines a small set of formatting rules for the portable representation of structured data. This document removes inconsistencies with other specifications of {JSON}, repairs specification errors, and offers experience-based interoperability guidance.},
	number = {{RFC} 8259},
	institution = {Internet Engineering Task Force},
	type = {Request for Comments},
	author = {Bray, Tim},
	urldate = {2022-02-07},
	date = {2017-12},
	doi = {10.17487/RFC8259},
	note = {Num Pages: 16},
	file = {Full Text PDF:/home/shalen/Zotero/storage/32LXI8UQ/Bray - 2017 - The JavaScript Object Notation (JSON) Data Interch.pdf:application/pdf},
}

@online{unity_technologies_unity_nodate-1,
	title = {Unity Manual: {IL}2CPP Overview},
	url = {https://docs.unity3d.com/Manual/IL2CPP.html},
	author = {{Unity Technologies}},
	urldate = {2022-02-07},
	langid = {english},
	file = {Snapshot:/home/shalen/Zotero/storage/2L8CCPCF/IL2CPP.html:text/html},
}

@online{noauthor_sockets_nodate,
	title = {Sockets - Hololens {UDP} Server never receives message},
	url = {https://stackoverflow.com/questions/44034118/hololens-udp-server-never-receives-message},
	titleaddon = {Stack Overflow},
	urldate = {2022-02-07},
	file = {Snapshot:/home/shalen/Zotero/storage/JADS3JQF/58834417.html:text/html},
}

@online{osthege_datagramsocketmessagereceived_2017,
	title = {{DatagramSocket}.{MessageReceived} does not trigger on incoming broadcast messages},
	url = {https://social.msdn.microsoft.com/Forums/en-US/41fa63c9-c19f-4fa3-8349-fad8e465af3c/uwp-datagramsocketmessagereceived-does-not-trigger-on-incoming-broadcast-messages?forum=wpdevelop},
	author = {Osthege, Michael},
	urldate = {2022-02-07},
	date = {2017},
	file = {[UWP] DatagramSocket.MessageReceived does not trigger on incoming broadcast messages:/home/shalen/Zotero/storage/GQ8VK6JE/uwp-datagramsocketmessagereceived-does-not-trigger-on-incoming-broadcast-messages.html:text/html},
}

@online{microsoft_corporation_initial_nodate,
	title = {Initial setup of World Locking Tools},
	url = {https://microsoft.github.io/MixedReality-WorldLockingTools-Unity/DocGen/Documentation/HowTos/InitialSetup.html#a-warning-note-on-installation-path-length},
	author = {{Microsoft Corporation}},
	urldate = {2022-02-07},
	file = {Initial setup of World Locking Tools | World Locking Tools for Unity Documentation:/home/shalen/Zotero/storage/ALBVD347/InitialSetup.html:text/html},
}

@online{finch_qr_nodate,
	title = {{QR} codes don't scan on first run},
	url = {https://github.com/microsoft/MixedReality-WorldLockingTools-Samples/issues/20},
	abstract = {Repro steps: Uninstall the {QRSpacePin} app (if it is installed). Build, deploy, and run the {QRSpacePin} app. Note that the Enumeration complete event never appears on the {SimpleConsole}. Note that no ...},
	titleaddon = {{GitHub}},
	author = {Finch, Mark},
	urldate = {2022-02-07},
	langid = {english},
	file = {Snapshot:/home/shalen/Zotero/storage/8M9ZJY7U/20.html:text/html},
}

@misc{hata_cs231a_nodate,
	title = {{CS}231A Course Notes 1: Camera Models},
	url = {http://web.stanford.edu/class/cs231a/course_notes/01-camera-models.pdf},
	author = {Hata, Kenji and Savarese, Silvio},
	urldate = {2022-02-08},
}

@article{zhang_flexible_2000,
	title = {A flexible new technique for camera calibration},
	volume = {22},
	issn = {1939-3539},
	doi = {10.1109/34.888718},
	abstract = {We propose a flexible technique to easily calibrate a camera. It only requires the camera to observe a planar pattern shown at a few (at least two) different orientations. Either the camera or the planar pattern can be freely moved. The motion need not be known. Radial lens distortion is modeled. The proposed procedure consists of a closed-form solution, followed by a nonlinear refinement based on the maximum likelihood criterion. Both computer simulation and real data have been used to test the proposed technique and very good results have been obtained. Compared with classical techniques which use expensive equipment such as two or three orthogonal planes, the proposed technique is easy to use and flexible. It advances 3D computer vision one more step from laboratory environments to real world use.},
	pages = {1330--1334},
	number = {11},
	journaltitle = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	author = {Zhang, Z.},
	date = {2000-11},
	note = {Conference Name: {IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Calibration, Cameras, Closed-form solution, Computer simulation, Computer vision, Layout, Lenses, Maximum likelihood estimation, Nonlinear distortion, Testing},
	file = {IEEE Xplore Abstract Record:/home/shalen/Zotero/storage/CJLWK8T3/888718.html:text/html},
}

@article{labatut_robust_2009,
	title = {Robust and Efficient Surface Reconstruction From Range Data},
	volume = {28},
	issn = {1467-8659},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-8659.2009.01530.x},
	doi = {10.1111/j.1467-8659.2009.01530.x},
	abstract = {We describe a robust but simple algorithm to reconstruct a surface from a set of merged range scans. Our key contribution is the formulation of the surface reconstruction problem as an energy minimisation problem that explicitly models the scanning process. The adaptivity of the Delaunay triangulation is exploited by restricting the energy to inside/outside labelings of Delaunay tetrahedra. Our energy measures both the output surface quality and how well the surface agrees with soft visibility constraints. Such energy is shown to perfectly fit into the minimum s − t cuts optimisation framework, allowing fast computation of a globally optimal tetrahedra labeling, while avoiding the “shrinking bias” that usually plagues graph cuts methods. The behaviour of our method confronted to noise, undersampling and outliers is evaluated on several data sets and compared with other methods through different experiments: its strong robustness would make our method practical not only for reconstruction from range data but also from typically more difficult dense point clouds, resulting for instance from stereo image matching. Our effective modeling of the surface acquisition inverse problem, along with the unique combination of Delaunay triangulation and minimum s − t cuts, makes the computational requirements of the algorithm scale well with respect to the size of the input point cloud.},
	pages = {2275--2290},
	number = {8},
	journaltitle = {Computer Graphics Forum},
	author = {Labatut, P. and Pons, J.-P. and Keriven, R.},
	urldate = {2022-02-08},
	date = {2009},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-8659.2009.01530.x},
	keywords = {Delaunay triangulation, I.3.5 Computer Graphics: Computational Geometry and Object Modeling, minimum s – t cut, surface reconstruction},
	file = {Submitted Version:/home/shalen/Zotero/storage/2RX2AWFZ/Labatut et al. - 2009 - Robust and Efficient Surface Reconstruction From R.pdf:application/pdf;Snapshot:/home/shalen/Zotero/storage/H4KT3JFZ/j.1467-8659.2009.01530.html:text/html},
}

@inproceedings{jancosek_multi-view_2011,
	title = {Multi-view reconstruction preserving weakly-supported surfaces},
	doi = {10.1109/CVPR.2011.5995693},
	abstract = {We propose a novel method for the multi-view reconstruction problem. Surfaces which do not have direct support in the input 3D point cloud and hence need not be photo-consistent but represent real parts of the scene (e.g. low-textured walls, windows, cars) are important for achieving complete reconstructions. We augmented the existing Labatut {CGF} 2009 method with the ability to cope with these difficult surfaces just by changing the t-edge weights in the construction of surfaces by a minimal s-t cut. Our method uses Visual-Hull to reconstruct the difficult surfaces which are not sampled densely enough by the input 3D point cloud. We demonstrate importance of these surfaces on several real-world data sets. We compare our improvement to our implementation of the Labatut {CGF} 2009 method and show that our method can considerably better reconstruct difficult surfaces while preserving thin structures and details in the same quality and computational time.},
	eventtitle = {{CVPR} 2011},
	pages = {3121--3128},
	booktitle = {{CVPR} 2011},
	author = {Jancosek, Michal and Pajdla, Tomas},
	date = {2011-06},
	note = {{ISSN}: 1063-6919},
	keywords = {Cameras, Face, Image reconstruction, Noise, Surface reconstruction, Surface texture, Three dimensional displays},
	file = {IEEE Xplore Abstract Record:/home/shalen/Zotero/storage/2Z387NBU/5995693.html:text/html},
}

@online{noauthor_epsg4326_nodate,
	title = {{EPSG}:4326},
	url = {http://epsg.io/4326},
	abstract = {{EPSG}:4326 Geodetic coordinate system for World},
	urldate = {2022-03-05},
	langid = {english},
	file = {Snapshot:/home/shalen/Zotero/storage/LYNDFZ7P/4326.html:text/html},
}